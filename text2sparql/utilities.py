# -*- coding: utf-8 -*-
"""utilities.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WoUAlLOEP7oIR4VBKVF36HdVGayL84Rf
"""

# Step 1: Query Template classification

def clean_query(query):
  doc = nlp(query)
  
  #Lemmatization
  lemmatized_query=""
  for token in doc:
    if "'" not in token.text:
      lemmatized_query=lemmatized_query+token.lemma_+" "
    else:
      lemmatized_query=lemmatized_query+token.text+" "

  # Remove punctuations
  cleaned_query=re.sub(r"[^\w\d'\s]+","",lemmatized_query)
  return cleaned_query

def get_ft_vectors(cleaned_query):
  # return embedder(cleaned_query)
  tensor_list= finbert.sentence_vector(cleaned_query)
  vector=[]
  for t in tensor_list:
    vector.append(t.numpy())
  return vector


def template_classification(query):
  bert_class=int(template_matching_model([query])[0])
  if bert_class == 4 or bert_class== 5:
    sql_template=bert_template[bert_class]
    return bert_class,1,sql_template

  else:
    # print("out")
    cleaned_query=clean_query(query)
    query_vector=pd.DataFrame(get_ft_vectors(cleaned_query)).T
    #print("got vector")
    label=XGB_template_classifier.predict(query_vector)[0]
    # print(label)
    prob=np.max(XGB_template_classifier.predict_proba(query_vector))
    if prob>=0.75:
     
      n_entities=XGB_template[label][0]
      # print(n_entities)
      sparql_templates=XGB_template[label][1:]
      # print(sparql_templates)
      return label,n_entities, sparql_templates
    else:
      return 1,100,"SELECT DISTINCT ?p ?o WHERE { <http://crypto.org/SUBJECT_CLASS/SUBJECT> ?p ?o.}"


# Step 2: Relation extraction

def clean_query_for_sim(cleaned_query):
  doc = nlp(cleaned_query)
  hyper_cleaned_query=""

  for token in doc:
    if token.pos_ == "VERB":
      hyper_cleaned_query=hyper_cleaned_query+token.lemma_+" "

  if hyper_cleaned_query=="":
    for token in doc:
      if token.pos_ == "NOUN":
        hyper_cleaned_query=hyper_cleaned_query+token.lemma_+" "

  return hyper_cleaned_query.strip()

def get_finbert_embeddings(text):
  if len(text.split())>1:
    tensor_list= finbert.sentence_vector(text)
  else :
    tensor_list= finbert.word_vector(text)

  vector=[]
  for t in tensor_list:
    vector.append(t.numpy())
  return vector

def get_single_relation(query):
  hyper_cleaned_query=clean_query_for_sim(query)
  fuzzy_score=[]
  for relation in relations:
    score=fuzz.token_set_ratio(hyper_cleaned_query,relation)
    fuzzy_score.append(score)
  if max(fuzzy_score)>85:
    #print(relation,hyper_cleaned_query,score)
    relation= relations[fuzzy_score.index(max(fuzzy_score))] 
    return relation

  query_finbert_vector=get_finbert_embeddings(hyper_cleaned_query)
  query_relation_score=[]
  for i,embedding in enumerate(relation_embeddings):
    score=1-cosine(embedding, query_finbert_vector)
    query_relation_score.append(score)
    #print(relations[i],score)

  #print(max(query_relation_score))
  if max(query_relation_score)>0.80:
    relation= relations[query_relation_score.index(max(query_relation_score))]
    
    return relation
  else:
    return "Not found"


# Step 3: Entity extraction


def map_entity(extracted_entity):
  
  fuzzy_score=[]
  for onto_entity in entities_ontology:
    score=fuzz.token_set_ratio(onto_entity,extracted_entity)
    fuzzy_score.append(score)
  
  if max(fuzzy_score)>85:
    #print(relation,hyper_cleaned_query,score)
    onto_entity= entities_ontology[fuzzy_score.index(max(fuzzy_score))] 
    return onto_entity
 

  entity_finbert_vector=get_finbert_embeddings(extracted_entity)
  # print(len(entity_finbert_vector))
  entity_onto_score=[]
  for i,embedding in enumerate(entity_embeddings):
    # print(len(embedding))
  #  if len(embedding)==3:
      # print(embedding)
      # print(entities[i])
      score=1-cosine(embedding, entity_finbert_vector)
      entity_onto_score.append(score)
    #print(relations[i],score)

  #print(max(query_relation_score))
  if max(entity_onto_score)>0.80:
    entity= entities_ontology[entity_onto_score.index(max(entity_onto_score))]
    return entity
  else:
    return "Not found"

# For templates 2 and 4
def get_single_entity(query):
  # If NER exists that's the entity - simply try mapping it to a class and entity
  doc=ner(query)
  if len(doc.ents)>0:
    entity=str(doc.ents[0])
  else:
    # extract NNPs/NNs
    entity=""
    for token in doc:
      if token.tag_=="NN" or token.tag_=="NNP":
        entity=entity+token.lemma_+" "
  
  if entity=="":
    return "Not found"
  else:
    #print(entity)
    mapped_entity=map_entity(entity)
    
  return mapped_entity


# For template 1
def get_entities(query):
  # If NER exists that's the entity - simply try mapping it to a class and entity
  entities=[]
  doc=ner(query)
  if len(doc.ents)>0:
    for entity in doc.ents:
      entities.append(str(entity))

  else:
    # extract NNPs/NNs
    
    for token in doc:
      if token.tag_=="NN" or token.tag_=="NNP":
        entities.append(token.lemma_)
  if len(entities)==0:
    return "Not found"
  else:
    mapped_entities=[]
    for entity in entities:
      mapped_entities.append(map_entity(entity))
  
  # Deleting entities for which no mapped entity could be found in Ontology
  final_mapped_entities=[]
  for mapped_entity in mapped_entities:
    if mapped_entity!="Not found":
      final_mapped_entities.append(mapped_entity)

    
  return final_mapped_entities