{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngvljtCyfZnq"
      },
      "outputs": [],
      "source": [
        "!pip install allennlp==2.1.0 allennlp-models==2.1.0\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcG-mDCzw9um"
      },
      "outputs": [],
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "import stanza\n",
        "stanza.install_corenlp()\n",
        "from stanza.server import CoreNLPClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhWXy-QYLi0i"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import json\n",
        "import os\n",
        "import collections\n",
        "import string\n",
        "\n",
        "WORD = re.compile(r\"\\w+\")\n",
        "coref = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/GoogleDrive/')\n",
        "# data_dir = '/content/GoogleDrive/MyDrive/results/meta-transcript-readable-pymupdf.txt'\n",
        "!unzip cleaned_json.zip\n",
        "data_dir = '/content/cleaned_json'"
      ],
      "metadata": {
        "id": "hp6ulGgu1u01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_UKdzmZLl_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ad044f-a531-4eae-9f01-f0aa23284e55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Cosine similarity between two vectors\n",
        "def get_cosine(vec1, vec2):\n",
        "  intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "  numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "  sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
        "  sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
        "  denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "  if not denominator:\n",
        "      return 0.0\n",
        "  else:\n",
        "      return float(numerator) / denominator\n",
        "\n",
        "# Convert string to a vector\n",
        "def text_to_vector(text):\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "# check if 2 triples exist with similar subject, object, predicate longer subject is taken\n",
        "def subset_phrase(triples, simScore):\n",
        "  n = len(triples)\n",
        "  new_triple = triples[:]\n",
        "  for i in range(n):\n",
        "    firstTri = triples[i]\n",
        "    for j in range(i + 1, n):\n",
        "      secondTri = triples[j]\n",
        "      text1 = firstTri[0] + \" \" + firstTri[1] + \" \" + firstTri[2] + \" \" + firstTri[3]\n",
        "      text2 = secondTri[0] + \" \" + secondTri[1] + \" \" + secondTri[2] + \" \" + secondTri[3]\n",
        "      vector1 = text_to_vector(text1)\n",
        "      vector2 = text_to_vector(text2)\n",
        "      # Doing the above eliminates worrying about scenarios of exactly the same subject, object or predicate\n",
        "      if get_cosine(vector1, vector2) >= simScore:\n",
        "        # temp = firstTri if len(firstTri[0]) > len(secondTri[0]) else secondTri #can make this based on the subject, not text\n",
        "        temp = firstTri if len(text1) > len(text2) else secondTri\n",
        "        if temp == secondTri:\n",
        "          if firstTri in new_triple:\n",
        "            new_triple.remove(firstTri)\n",
        "        elif secondTri in new_triple:\n",
        "            new_triple.remove(secondTri)\n",
        "  return new_triple\n",
        "\n",
        "\n",
        "def min_char_count(triple):\n",
        "    subject = triple[0]\n",
        "    predicate = triple[1]\n",
        "    object_ = triple[2]\n",
        "    if (len(subject)) < 2 or (len(predicate) < 2) or (len(object_) < 2):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def duplicate(triple):\n",
        "    # duplicants should not exist in subject and object\n",
        "    subject = triple[0]\n",
        "    predicate = triple[1]\n",
        "    object_ = triple[2]\n",
        "        \n",
        "    def count_duplicate(string_input):\n",
        "        split_list = string_input.split()\n",
        "        word_counts = collections.Counter(split_list)\n",
        "        for word, count in word_counts.items():\n",
        "            if count > 1:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    if any([count_duplicate(subject), count_duplicate(predicate), count_duplicate(object_)]):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def special_characters(triple):\n",
        "    subject = triple[0]\n",
        "    predicate = triple[1]\n",
        "    object_ = triple[2]\n",
        "  \n",
        "    def find_sc(string_input):\n",
        "        for s in string_input:    \n",
        "            if s.isalpha():\n",
        "                pass\n",
        "            elif s.isdigit():\n",
        "                pass\n",
        "            elif s in string.punctuation:\n",
        "                # sc.append(s)\n",
        "                return True\n",
        "\n",
        "    if any([find_sc(subject), find_sc(predicate), find_sc(object_)]):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def special_characters(triple):\n",
        "    subject = triple[0]\n",
        "    predicate = triple[1]\n",
        "    object_ = triple[2]\n",
        "  \n",
        "    def find_sc(string_input):\n",
        "        for s in string_input:    \n",
        "            if s.isalpha():\n",
        "                pass\n",
        "            elif s.isdigit():\n",
        "                pass\n",
        "            elif s in string.punctuation:\n",
        "                # sc.append(s)\n",
        "                return True\n",
        "\n",
        "    if any([find_sc(subject), find_sc(predicate), find_sc(object_)]):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# return true or false if triple is a valid triple\n",
        "def filter_triple(triple):\n",
        "  DAY_OF_THE_WEEK = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
        "  subject = triple[0]\n",
        "  predicate = triple[1]\n",
        "  object_ = triple[2]\n",
        "  doc_sub = nlp(subject)\n",
        "  doc_obj = nlp(object_)\n",
        "  doc_pred = nlp(predicate)\n",
        "  subject_pos = [token.pos_ for token in doc_sub] #all parts of speech\n",
        "  object_pos = [token.pos_ for token in doc_obj] #all parts of speech\n",
        "  predicate_pos = [token.pos_ for token in doc_pred] #all parts of speech\n",
        "  all_words_day_week = [True if word.lower() in DAY_OF_THE_WEEK else False for word in subject.split()]\n",
        "  contains_day_of_week = any(all_words_day_week) #performs OR operation of booleans in list\n",
        "  if 'NOUN' not in subject_pos and 'PROPN' not in subject_pos:\n",
        "    return False\n",
        "  elif contains_day_of_week:\n",
        "    return False\n",
        "  elif ('VERB' in subject_pos) or ('VERB' in object_pos):\n",
        "    return False\n",
        "  elif any([\"PRON\" in subject_pos,\"PRON\" in predicate_pos, \"PRON\" in object_pos]):\n",
        "    return False\n",
        "  elif subject == object_:\n",
        "    return False\n",
        "  elif not special_characters(ele):\n",
        "    return False\n",
        "  elif not duplicate(ele):\n",
        "    return False\n",
        "  elif not min_char_count(ele):\n",
        "    return False\n",
        "  elif len(list(filter(lambda x: x != 'PUNCT', subject_pos))) > 3:\n",
        "    return False\n",
        "  elif len(list(filter(lambda x: x != 'PUNCT', object_pos))) > 3:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "filter_triple((\"Tuesday\", \"is\", \"hard\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def addTriple(document, simThresh):\n",
        "  allTriples = []\n",
        "  triples = []\n",
        "  for sentence in document['sentences']:\n",
        "    # Add temporal relation\n",
        "    cursentTemp = \"\"\n",
        "    for ele in sentence['entitymentions']:\n",
        "      if ele['ner'] == \"DATE\":\n",
        "        temp = ele['text'] \n",
        "        cursentTemp = temp if len(temp) > len(cursentTemp) else cursentTemp    \n",
        "    sentTrip = []\n",
        "    for triple in sentence['openie']:\n",
        "      if filter_triple((triple['subject'], triple['relation'], triple['object'])):\n",
        "        sentTrip.append((triple['subject'], triple['relation'], triple['object'], cursentTemp))\n",
        "    valid_triples = subset_phrase(sentTrip, simThresh) #more similarity score increases the number of sentences retrieved since we're making sure sentences are extremely close in order to drop the shorter one. \n",
        "    triples.append(valid_triples)\n",
        "    \n",
        "  triples = [item for sublist in triples for item in sublist]\n",
        "  for ele in triples:\n",
        "    tmp_arg = ele[3]\n",
        "    if len(tmp_arg) > 0:\n",
        "      allTriples.append((ele[2], ele[1], ele[3]))\n",
        "    allTriples.append((ele[0], ele[1], ele[2]))\n",
        "  return allTriples"
      ],
      "metadata": {
        "id": "YpXn-yhy2fQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESS TEXT\n",
        "def clean_text(sentence):\n",
        "  \"\"\"\n",
        "  Input sentence: Raw sentence\n",
        "  Output sentence: cleaned sentence with - \n",
        "                  (i) no extra whitespaces, no new lines, no tabs\n",
        "                  (ii) lemmatized sentence\n",
        "                  (iii) generic sentences with no NER are returned as empty string\n",
        "\n",
        "  \"\"\"\n",
        "  doc=nlp(sentence)\n",
        "  # lemmatization\n",
        "  lemmatized_sentence=\"\"\n",
        "  for token in doc:\n",
        "    if token.lemma_ !=\"-PRON-\":\n",
        "      lemmatized_sentence=lemmatized_sentence+token.lemma_+\" \"\n",
        "    else:\n",
        "      if token.lemma_ == \" . \":\n",
        "        print(\"MAD OO\")\n",
        "      lemmatized_sentence=lemmatized_sentence+str(token)+\" \"\n",
        "  sentence=lemmatized_sentence[:-1]\n",
        "\n",
        "  # removing whitespace, /n, tabs\n",
        "  sentence = sentence.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ')\n",
        "  pattern = re.compile(r'\\s+') \n",
        "  Without_whitespace = re.sub(pattern, ' ', sentence)\n",
        "  # There are some instances where there is no space after '?' & ')', \n",
        "  # So I am replacing these with one space so that It will not consider two words as one token.\n",
        "  sentence = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
        "  return sentence\n",
        "\n",
        "\n",
        "text = \"Ridwan is not a person. He is nice.\"\n",
        "print(clean_text(text))"
      ],
      "metadata": {
        "id": "to-KKhEwTFKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3a31d3-7616-4305-9075-fd28d728c275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridwan be not a person . He be nice .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load files from source document\n",
        "allTriples = []\n",
        "files = sorted(os.listdir(data_dir))\n",
        "# allText = \"\"\n",
        "for file in files:\n",
        "  with open(f'{data_dir}/{file}') as f:\n",
        "    d = json.load(f)\n",
        "  text = d['text']\n",
        "  all_authors = \"\"\n",
        "  for ele in d['auhtors']:\n",
        "    all_authors += ele + \", \"\n",
        "  allTriples.append((all_authors[:-2], 'author(s)', d['title']))\n",
        "  if len(text) > 0:\n",
        "    text = clean_text(text)\n",
        "    coref_text = coref.coref_resolved(document=text)\n",
        "    with CoreNLPClient(timeout=150000000, be_quiet=False, annotators=['ner', 'openie'], memory='25G', endpoint='http://localhost:9001') as client:\n",
        "      document = client.annotate(coref_text, output_format='json', properties={\"openie.triple.all_nominals\": True})\n",
        "      allTriples.extend(addTriple(document, 0.5))\n",
        "    # allText += coref_text + \" \""
      ],
      "metadata": {
        "id": "Sa_S7yJrdzNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dr = {\"results\" : allTriples}\n",
        "json_object = json.dumps(dr)\n",
        "with open(\"stanfordopenie.json\", \"w\") as f:\n",
        "  f.write(json_object)"
      ],
      "metadata": {
        "id": "dPF4uFQje_yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib\n",
        "from rdflib import Graph\n",
        "import pprint\n",
        "g = Graph()\n",
        "g.parse(\"/content/demo.nt\")"
      ],
      "metadata": {
        "id": "sPuIOd22gUQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for stmt in g:\n",
        "    pprint.pprint(stmt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSZu_2Yxg56a",
        "outputId": "576b731f-0189-4e13-937c-101366bb5f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(rdflib.term.URIRef('http://example.com/drewp'),\n",
            " rdflib.term.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
            " rdflib.term.URIRef('http://xmlns.com/foaf/0.1/Person'))\n",
            "(rdflib.term.URIRef('http://example.com/drewp'),\n",
            " rdflib.term.URIRef('http://example.com/says'),\n",
            " rdflib.term.Literal('Hello World'))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}