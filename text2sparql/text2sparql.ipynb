{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-nJb5TU3Rq0",
        "outputId": "179e2813-720d-479d-f24d-370f7e24178d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnIJjVBn8sUr",
        "outputId": "603c4e9a-d0c5-48a8-d56e-a57690c14296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Capstone/t2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-28 18:05:14.23 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/kbqa/wikidata/query_prediction_eng.tar.gz download because of matching hashes\n",
            "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/kbqa/wikidata/query_prediction_eng.tar.gz download because of matching hashes\n",
            "2022-11-28 18:05:14.624 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/kbqa/wikidata/query_prediction_eng.pickle download because of matching hashes\n",
            "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/kbqa/wikidata/query_prediction_eng.pickle download because of matching hashes\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at haisongzhang/roberta-tiny-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd \"/content/gdrive/MyDrive/Capstone/t2s/\" # change path to files here\n",
        "from utilities import *\n",
        "!pip install deeppavlov\n",
        "!pip install transformers\n",
        "!pip install finbert-embedding\n",
        "!pip install spacy\n",
        "!pip install xgboost\n",
        "!pip install sister\n",
        "!pip install fuzzywuzzy\n",
        "!pip install spacy[transformers]\n",
        "!python -m spacy download en_core_web_trf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk2nDOTOmkTf",
        "outputId": "5b908cc2-b7d9-499f-ea12-5e1e61f40a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('separate', 'YIELD')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SELECT (count(*) as? count) WHERE {<http://crypto.org/PERSON/Ramit_Sawhney><http://crypto.org/YIELD/separate> ?o.}',\n",
              " 'SELECT (count(*) as? count) WHERE {?s <http://crypto.org/YIELD/separate> <http://crypto.org/PERSON/Ramit_Sawhney>.}']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def text2sparql(query,ontology):\n",
        "\n",
        "  #if\n",
        "  sparql_query_list=[]\n",
        "\n",
        "  template_id,n_entities, query_templates=template_classification(query)#\n",
        "  #print(template_id)\n",
        "  if template_id==4 or template_id==2:\n",
        "    #print(get_single_relation(query,ontology))\n",
        "    relation,relation_class=get_single_relation(query,ontology)#\n",
        "    #print(relation)\n",
        "\n",
        "    entity,entity_class=get_single_entity(query,ontology)#\n",
        "    \n",
        "    \n",
        "    if (entity!=\"Not found\" and relation!=\"Not found\" ):\n",
        "      for query_template in query_templates:\n",
        "        sparql_query_list.append(query_template.replace(\"ENTITY_CLASS\",entity_class.replace(\" \",\"_\")).replace(\"ENTITY\",entity.replace(\" \",\"_\")).replace(\"RELATION_CLASS\",relation_class.replace(\" \",\"_\")).replace(\"RELATION\",relation.replace(\" \",\"_\")))\n",
        "    else:\n",
        "      sparql_query_list.append(\"Not found\")\n",
        "\n",
        "  if template_id==1:\n",
        "    entities, entities_class=get_entities(query)\n",
        "    if len(entities)!=0:\n",
        "      for i,entity in enumerate(entities):\n",
        "        entity_class=entities_class[i]\n",
        "        sparql_query_list.append(query_template.replace(\"ENTITY_CLASS\",entity_class.replace(\" \",\"_\")).replace(\"ENTITY\",entity.replace(\" \",\"_\")))\n",
        "    else:\n",
        "      sparql_query_list.append(\"Not found\")\n",
        "\n",
        "  \n",
        "  return sparql_query_list\n",
        "\n",
        "\n",
        "query1=\"What are the papers that Ramit Sawhney authored?\"\n",
        "query2=\"How many papers has Ramit Sawhney authored?\"\n",
        "\n",
        "text2sparql(query2,\"stanford\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}