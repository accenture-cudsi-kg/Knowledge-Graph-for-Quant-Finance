{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# REBEL"
      ],
      "metadata": {
        "id": "GcpZAth-_BAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking REBEL model"
      ],
      "metadata": {
        "id": "WxQEwjBkJWjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niFrGGPa7wsS",
        "outputId": "128cde0b-789f-42eb-b772-b1768523389a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 37.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.0 tokenizers-0.12.1 transformers-4.22.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')\n",
        "\n",
        "# We need to use the tokenizer manually since we need special tokens.\n",
        "str_data = \"\"\"Stocks continued to recover from oversold levels, with traders weighing renewed speculation that global central banks could moderate their hawkish stance to prevent a hard landing.\n",
        "The S&P 500 is having its best two-day surge since November 2020. On top of the obvious short squeeze, a soft economic reading gave bulls a reason for hope when it comes to Federal Reserve policy. \n",
        "US job openings sank to a 14-month low -- which may fit well with a central bank that’s very concerned about a hot jobs market. Bond yields fell with the dollar.\n",
        "Traders also kept a close eye on Tesla Inc., which pared gains substantially on news that Elon Musk is proposing to buy Twitter Inc. for the original offer price of $54.20 a share.\n",
        "The debate over peak hawkishness has intensified after a dovish surprise from Australia’s central bank and bond buying by the Bank of England.\n",
        "The idea of a Fed pivot, however, has been met with a lot of skepticism. \n",
        "For one, there’s the perception that not much has fundamentally changed to sway officials from their primary goal to knock down inflation. \n",
        "Then, there’s the fact that stock pessimism reached such extreme levels that a bounce would be just a matter of when. \n",
        "For markets that had been “nearly one-sided,” the liquidation of those positions is a big reason to squeeze in the other direction so vigorously, said Fawad Razaqzada at City Index and Forex.com.\"\"\"\n",
        "\n",
        "extracted_text = triplet_extractor.tokenizer.batch_decode([triplet_extractor(str_data, return_tensors=True, return_text=False)[0][\"generated_token_ids\"]])\n",
        "\n",
        "print(extracted_text[0])\n",
        "\n",
        "# Function to parse the generated text and extract the triplets\n",
        "def extract_triplets(text):\n",
        "    triplets = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
        "    return triplets\n",
        "extracted_triplets = extract_triplets(extracted_text[0])\n",
        "print(extracted_triplets)"
      ],
      "metadata": {
        "id": "kak3JOlA41O2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027a38e7-7549-4d16-917d-d1c5ed456f69"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s><triplet> Elon Musk <subj> Twitter Inc. <obj> owner of <triplet> Twitter Inc. <subj> Elon Musk <obj> owned by</s>\n",
            "[{'head': 'Elon Musk', 'type': 'owner of', 'tail': 'Twitter Inc.'}, {'head': 'Twitter Inc.', 'type': 'owned by', 'tail': 'Elon Musk'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_triplets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrt8-92K8nmU",
        "outputId": "03f6cd15-cb7c-4427-8850-7728a8eb8633"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'head': 'Elon Musk', 'type': 'owner of', 'tail': 'Twitter Inc.'},\n",
              " {'head': 'Twitter Inc.', 'type': 'owned by', 'tail': 'Elon Musk'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}